TomatoEnv:
  total_timesteps: 2_000_000
  n_envs: 8
  policy: MlpPolicy
  learning_rate: 7.e-4
  buffer_size: 576_100  
  learning_starts: 57_610 # let the agents randomly explore for 10 episodes.
  batch_size: 128
  tau: 0.0135
  gamma: 0.9631
  ent_coef: auto
  train_freq: 50
  gradient_steps: 10
  action_noise: 
    normalactionnoise:
      sigma: 0.05
  replay_buffer_class: null
  replay_buffer_kwargs: null
  optimize_memory_usage: False
  target_update_interval: 1
  use_sde: False
  sde_sample_freq: -1
  use_sde_at_warmup: False
  stats_window_size: 100

  policy_kwargs: {net_arch: {pi: [256, 256, 256], qf: [512, 512, 512]},
                  optimizer_class: adam,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: silu,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }
LettuceEnv:
  total_timesteps: 2_000_000
  n_envs: 8
  policy: MlpPolicy
  learning_rate: 7.e-4
  buffer_size: 576_100  
  learning_starts: 57_610 # let the agents randomly explore for 10 episodes.
  batch_size: 128
  tau: 0.0135
  gamma: 0.9631
  ent_coef: auto
  train_freq: 50
  gradient_steps: 10
  action_noise: 
    normalactionnoise:
      sigma: 0.05
  replay_buffer_class: null
  replay_buffer_kwargs: null
  optimize_memory_usage: False
  target_update_interval: 1
  use_sde: False
  sde_sample_freq: -1
  use_sde_at_warmup: False
  stats_window_size: 100

  policy_kwargs: {net_arch: {pi: [256, 256, 256], qf: [512, 512, 512]},
                  optimizer_class: adam,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: silu,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }

HydroponicLettuceEnv:
  total_timesteps: 1_500_000        # Increased for complex hydroponic control
  n_envs: 8
  policy: MlpPolicy
  learning_rate: 5.e-4              # Lower learning rate for stability
  buffer_size: 576_100  
  learning_starts: 57_610           # Let the agents randomly explore for 10 episodes
  batch_size: 256                   # Larger batch for stability
  tau: 0.01                         # Faster target network updates
  gamma: 0.98                       # Higher discount for long-term optimization
  ent_coef: auto
  train_freq: 50
  gradient_steps: 15                # More gradient steps for complex learning
  action_noise: 
    normalactionnoise:
      sigma: 0.03                   # Lower noise for precise hydroponic control
  replay_buffer_class: null
  replay_buffer_kwargs: null
  optimize_memory_usage: False
  target_update_interval: 1
  use_sde: False
  sde_sample_freq: -1
  use_sde_at_warmup: False
  stats_window_size: 100

  policy_kwargs: {net_arch: {pi: [512, 512, 512, 256], qf: [512, 512, 512, 512]},
                  optimizer_class: adam,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: silu,
                  log_std_init: np.log(0.8) # Lower std for stable hydroponic control
          }



         
